{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33aefd94",
   "metadata": {},
   "source": [
    "# LangChain Message Types\n",
    "LangChain provides a structured way to simulate chat-style interactions using different types of messages. These are imported from <br><b>langchain_core.messages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a2b4f",
   "metadata": {},
   "source": [
    "types of messages used in LangChain — like <b>HumanMessage, <b>SystemMessage, and <b>AIMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad907cc",
   "metadata": {},
   "source": [
    "### HumanMessage : \n",
    "Represents a message coming from the user (you) — this is typically a question or prompt.\n",
    "eg : What is the capital of India?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4bc8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6h/lkrcqc7d0vj6gsp7yc3xwlbr0000gn/T/ipykernel_47769/4200144456.py:10: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  model = Ollama(model='gemma:2b')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is Delhi. It is the political and administrative center of the country.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the HumanMessage class to create human input for the model\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Step 2: Import the Ollama LLM (Large Language Model) wrapper from LangChain\n",
    "# This helps connect a local model running via Ollama\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Step 3: Initialize the Ollama model\n",
    "# Here we're using the 'gemma:2b' model. Make sure this model is available via Ollama on your machine.\n",
    "model = Ollama(model='gemma:2b')\n",
    "\n",
    "# Step 4: Create a prompt\n",
    "# We're wrapping our question in a HumanMessage object, as required by LangChain\n",
    "prompt = [HumanMessage('What is the capital of India?')]\n",
    "\n",
    "# Step 5: Send the prompt to the model and get the response\n",
    "output = model.invoke(prompt)\n",
    "\n",
    "# Step 6: Print the model's response\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68bb1e5",
   "metadata": {},
   "source": [
    "### SystemMessage: \n",
    "Used to define the context or instructions for the model. It helps guide the behavior of the LLM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34835da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! A **world** is a vast and complex place that encompasses all that exists, including all living things, all non-living things, and everything in between. It is a place of endless possibilities and endless wonder.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "#  Create the system instruction (this sets the assistant's behavior)\n",
    "system_instruction = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "# reate a user message (your actual question or task)\n",
    "user_question = HumanMessage(content=\"Can you explain what is world ?\")\n",
    "\n",
    "prompt = [system_instruction, user_question]\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68df247",
   "metadata": {},
   "source": [
    "### AI Message: \n",
    "Represents a response generated by the LLM (AI). This is usually returned after invoking the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f448b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of India is New Delhi.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import the AIMessage class\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "# Step 2: Create an AI message (simulated response from an assistant)\n",
    "ai_response = AIMessage(content=\"The capital of India is New Delhi.\")\n",
    "\n",
    "# Step 3: Print the AI message content\n",
    "print(ai_response.content)\n",
    "\n",
    "\n",
    "# This code does not involve a live model—it just creates an AIMessage object and prints its content,\n",
    "#  which is helpful for simulating or logging assistant replies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c27056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
